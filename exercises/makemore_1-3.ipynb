{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "\n",
    "all_letters = []\n",
    "for word in words:\n",
    "  for letter in word:\n",
    "    all_letters.append(letter)\n",
    "\n",
    "all_letters = sorted(list(set(all_letters)))\n",
    "print(all_letters)\n",
    "\n",
    "stoi = {letter: i+1 for i, letter in enumerate(all_letters)}\n",
    "itos = {i+1: letter for i, letter in enumerate(all_letters)}\n",
    "stoi[\".\"] = 0\n",
    "itos[0] = \".\"\n",
    "print(stoi)\n",
    "print(itos)\n",
    "\n",
    "N = np.zeros((len(stoi), len(stoi)), dtype=np.int64)\n",
    "\n",
    "print(N.shape)\n",
    "\n",
    "b = {}\n",
    "for word in words:\n",
    "  chs = [\".\"] + list(word) + [\".\"]\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    bigram = (ch1, ch2)\n",
    "    b[bigram] = b.get(bigram, 0) + 1\n",
    "\n",
    "sorted(list(b.items()), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "for bigram, count in b.items():\n",
    "  N[stoi[bigram[0]], stoi[bigram[1]]] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap=\"Blues\")\n",
    "for i in range(27):\n",
    "  for j in range(27):\n",
    "    chstr = itos[i] + itos[j]\n",
    "    plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"black\")\n",
    "    plt.text(j, i, N[i, j], ha=\"center\", va=\"top\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = N.astype(np.float64)\n",
    "P = N.sum(axis=1, keepdims=True)\n",
    "P = N / P\n",
    "\n",
    "P = torch.tensor(P)\n",
    "\n",
    "for _ in range(10):\n",
    "  i = 0\n",
    "  word = \"\"\n",
    "  while True:\n",
    "    i = torch.multinomial(P[i], num_samples=1, replacement=True).item()\n",
    "    word += itos[i]\n",
    "    if i == 0:\n",
    "      break\n",
    "  print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "\n",
    "X, Y, stoi, itos = load_data(\"names.txt\", block_size=block_size)\n",
    "\n",
    "X = torch.tensor(X[\"train\"])\n",
    "Y = torch.tensor(Y[\"train\"]).view(-1)\n",
    "print(\"X\", X.shape)\n",
    "print(\"Y\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Check if MPS is available (for Mac)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)\n",
    "C = torch.randn((27, embedding_dim)).to(device)\n",
    "C.requires_grad = True\n",
    "W1 = torch.randn((embedding_dim * block_size, hidden_dim)).to(device)\n",
    "W1.requires_grad = True\n",
    "b1 = torch.randn(hidden_dim).to(device)\n",
    "b1.requires_grad = True\n",
    "W2 = torch.randn((hidden_dim, 27)).to(device)\n",
    "W2.requires_grad = True\n",
    "b2 = torch.randn(27).to(device)\n",
    "b2.requires_grad = True\n",
    "params = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "iters = 30000\n",
    "lres = np.arange(1, 3, 2 / iters).astype(np.float64)\n",
    "lrs = 10 ** (-lres)\n",
    "\n",
    "for itr in range(iters):\n",
    "  indices = torch.randint(0, len(X), (batch_size,))\n",
    "  X_batch = X[indices]\n",
    "  Y_batch = Y[indices]\n",
    "  embeddings = C[X_batch]\n",
    "  embeddings = embeddings.view(-1, embedding_dim * block_size)\n",
    "  logits = embeddings @ W1 + b1\n",
    "  logits = F.tanh(logits)\n",
    "  logits = logits @ W2 + b2\n",
    "  for p in params:\n",
    "    p.grad = None\n",
    "  loss = F.cross_entropy(logits, Y_batch)\n",
    "  loss.backward()\n",
    "  lr = lrs[itr]\n",
    "  for p in params:\n",
    "    p.data -= lr * p.grad\n",
    "  if itr % 1000 == 0:\n",
    "    print(f\"iter {itr}, loss {loss.item()}, lr {lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "  x = torch.zeros(block_size, dtype=torch.int64).to(device)\n",
    "  word = \"\"\n",
    "  while True:\n",
    "    embeddings = C[x].view(-1, embedding_dim * block_size)\n",
    "    logits = embeddings @ W1 + b1\n",
    "    logits = F.tanh(logits)\n",
    "    logits = logits @ W2 + b2\n",
    "    softmax = F.softmax(logits, dim=1)\n",
    "    y = torch.multinomial(softmax[0], num_samples=1)\n",
    "    word += itos[y.item()]\n",
    "    if y.item() == 0:\n",
    "      break\n",
    "    x = torch.cat([x[1:], y])\n",
    "  print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, fan_in, fan_out, device=\"cpu\", name=None):\n",
    "    self.device = device\n",
    "    self.W = torch.randn((fan_in, fan_out)).to(device) / fan_in**0.5\n",
    "    self.W.requires_grad = True\n",
    "    self.b = torch.zeros(fan_out).to(device)\n",
    "    self.b.requires_grad = True\n",
    "    self.name = name\n",
    "\n",
    "  def forward(self, x):\n",
    "    self.out = x @ self.W + self.b\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    params = [self.W, self.b]\n",
    "    return params\n",
    "\n",
    "  def zero_grad(self):\n",
    "    self.W.grad = None\n",
    "    self.b.grad = None\n",
    "\n",
    "  def train(self):\n",
    "    self.training = True\n",
    "\n",
    "  def eval(self):\n",
    "    self.training = False\n",
    "\n",
    "class Tanh:\n",
    "  def __init__(self, name=None):\n",
    "    self.name = name\n",
    "\n",
    "  def forward(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "  def zero_grad(self):\n",
    "    pass\n",
    "\n",
    "  def train(self):\n",
    "    self.training = True\n",
    "\n",
    "  def eval(self):\n",
    "    self.training = False\n",
    "\n",
    "class Embedding:\n",
    "  def __init__(self, num_embeddings, embedding_dim, device=\"cpu\", name=None):\n",
    "    self.device = device\n",
    "    self.embedding = torch.randn((num_embeddings, embedding_dim)).to(device)\n",
    "    self.embedding.requires_grad = True\n",
    "    self.name = name\n",
    "\n",
    "  @property\n",
    "  def num_embeddings(self):\n",
    "    return self.embedding.shape[0]\n",
    "\n",
    "  @property\n",
    "  def embedding_dim(self):\n",
    "    return self.embedding.shape[1]\n",
    "\n",
    "  def forward(self, x):\n",
    "    block_size = x.shape[1]\n",
    "    return self.embedding[x].view(-1, self.embedding_dim * block_size)\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.embedding]\n",
    "\n",
    "  def zero_grad(self):\n",
    "    self.embedding.grad = None\n",
    "\n",
    "  def train(self):\n",
    "    self.training = True\n",
    "\n",
    "  def eval(self):\n",
    "    self.training = False\n",
    "\n",
    "class BatchNorm1d():\n",
    "  def __init__(self, num_features, eps=1e-5, momentum=0.1, device=\"cpu\", name=None):\n",
    "    self.device = device\n",
    "    self.num_features = num_features\n",
    "    self.name = name\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.W = torch.ones(num_features).to(device)\n",
    "    self.W.requires_grad = True\n",
    "    self.b = torch.zeros(num_features).to(device)\n",
    "    self.b.requires_grad = True\n",
    "    self.running_mean = torch.zeros(num_features).to(device)\n",
    "    self.running_var = torch.ones(num_features).to(device)\n",
    "    self.training = True\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.training:\n",
    "      mean = x.mean(dim=0)\n",
    "      var = x.var(dim=0)\n",
    "    else:\n",
    "      mean = self.running_mean\n",
    "      var = self.running_var\n",
    "    x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "    self.out = self.W * x_hat + self.b\n",
    "    if self.training:\n",
    "      self.running_mean = self.running_mean * (1 - self.momentum) + mean * self.momentum\n",
    "      self.running_var = self.running_var * (1 - self.momentum) + var * self.momentum\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.W, self.b]\n",
    "\n",
    "  def zero_grad(self):\n",
    "    self.W.grad = None\n",
    "    self.b.grad = None\n",
    "\n",
    "  def train(self):\n",
    "    self.training = True\n",
    "\n",
    "  def eval(self):\n",
    "    self.training = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "  def __init__(self, layers, name=None):\n",
    "    self.layers = layers\n",
    "    self.name = name\n",
    "\n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer.forward(x)\n",
    "    return x\n",
    "\n",
    "  def parameters(self):\n",
    "    params = []\n",
    "    for layer in self.layers:\n",
    "      params += layer.parameters()\n",
    "    return params\n",
    "\n",
    "  def zero_grad(self):\n",
    "    for layer in self.layers:\n",
    "      layer.zero_grad()\n",
    "\n",
    "  def train(self):\n",
    "    for layer in self.layers:\n",
    "      layer.train()\n",
    "\n",
    "  def eval(self):\n",
    "    for layer in self.layers:\n",
    "      layer.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Embedding(27, embedding_dim, name=\"emb\", device=device),\n",
    "  Linear(embedding_dim * block_size, hidden_dim, name=\"linear1\", device=device),\n",
    "  BatchNorm1d(num_features=hidden_dim, name=\"bn1\", device=device),\n",
    "  Tanh(),\n",
    "  Linear(hidden_dim, hidden_dim, name=\"linear2\", device=device),\n",
    "  BatchNorm1d(num_features=hidden_dim, name=\"bn2\", device=device),\n",
    "  Tanh(),\n",
    "  Linear(hidden_dim, hidden_dim, name=\"linear3\", device=device),\n",
    "  BatchNorm1d(num_features=hidden_dim, name=\"bn3\", device=device),\n",
    "  Tanh(),\n",
    "  Linear(hidden_dim, hidden_dim, name=\"linear4\", device=device),\n",
    "  BatchNorm1d(num_features=hidden_dim, name=\"bn4\", device=device),\n",
    "  Tanh(),\n",
    "  Linear(hidden_dim, hidden_dim, name=\"linear5\", device=device),\n",
    "  BatchNorm1d(num_features=hidden_dim, name=\"bn5\", device=device),\n",
    "  Tanh(),\n",
    "  Linear(hidden_dim, 27, name=\"linear2\", device=device),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "iters = 10000\n",
    "lres = np.arange(1, 3, 2 / iters).astype(np.float64)\n",
    "lrs = 10 ** (-lres)\n",
    "\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for itr in range(iters):\n",
    "  indices = torch.randint(0, len(X), (batch_size,))\n",
    "  X_batch = X[indices]\n",
    "  Y_batch = Y[indices]\n",
    "  logits = model.forward(X_batch)\n",
    "  model.zero_grad()\n",
    "  loss = F.cross_entropy(logits, Y_batch)\n",
    "  loss.backward()\n",
    "  lr = lrs[itr]\n",
    "  with torch.no_grad():\n",
    "    for p in model.parameters():\n",
    "      p.data -= lr * p.grad\n",
    "  if itr % 1000 == 0:\n",
    "    print(f\"iter {itr}, loss {loss.item()}, lr {lr}\")\n",
    "    for i, layer in enumerate(model.layers):\n",
    "      if isinstance(layer, Tanh):\n",
    "        t = layer.out\n",
    "        print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(model.layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out.cpu().detach().flatten()\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach().cpu(), hy.detach().cpu())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__})')\n",
    "plt.legend(legends)\n",
    "plt.title('activation distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for _ in range(10):\n",
    "  x = torch.zeros((1, block_size), dtype=torch.int64).to(device)\n",
    "  word = \"\"\n",
    "  while True:\n",
    "    logits = model.forward(x)\n",
    "    softmax = F.softmax(logits, dim=1)\n",
    "    y = torch.multinomial(softmax[0], num_samples=1)\n",
    "    word += itos[y.item()]\n",
    "    if y.item() == 0:\n",
    "      break\n",
    "    x[0] = torch.cat([x[0, 1:], y])\n",
    "  print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
